{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13266612,"sourceType":"datasetVersion","datasetId":8407008}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==== Kaggle Video Transcript to 10-Second CSV + 10-Minute Summaries ====\n\n# 1. Install libraries (run once)\n!pip install moviepy git+https://github.com/openai/whisper.git transformers torch --quiet\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 2. Find input video file\nimport os\ninput_dir = '/kaggle/input/rifatt'\nvideo_filename = 'CSE-Math4641-Lecture18(Recording)1.mkv'\nfor root, dirs, files in os.walk(input_dir):\n    for file in files:\n        if file.lower().endswith(('.mp4', '.mkv', '.mov', '.avi')):\n            video_filename = file\n            video_path = os.path.join(root, file)\nprint(f\"Found video: {video_filename} at {video_path}\")\n\n# 3. Extract audio from video\nfrom moviepy.editor import VideoFileClip\nprint(\"Extracting audio from video...\")\nvideo = VideoFileClip(video_path)\naudio_path = './extracted_audio.wav'\nvideo.audio.write_audiofile(audio_path, verbose=False, logger=None)\nvideo_duration = video.duration\nprint(f\"Audio extraction completed! Video duration: {video_duration:.2f} seconds\")\n\n# 4. Transcribe audio with Whisper (word timestamps)\nimport whisper\nprint(\"Loading Whisper model...\")\nmodel = whisper.load_model('base')\nprint(\"Transcribing audio with word-level timestamps...\")\nresult = model.transcribe(audio_path, word_timestamps=True)\nprint(\"Transcription completed!\")\n\n# 5. Build 10-second transcript chunks and save to CSV\nimport math\nimport pandas as pd\n\nn_secs = int(math.ceil(result['segments'][-1]['end']))\nseconds_per_chunk = 10\nchunks = []\nwords_by_sec = [[] for _ in range(n_secs+1)]\n\nfor segment in result['segments']:\n    for wi in segment.get('words', []):\n        word = wi['word']\n        start_sec = int(wi['start'])\n        end_sec = int(wi['end'])\n        for s in range(start_sec, end_sec+1):\n            if s < len(words_by_sec):\n                words_by_sec[s].append(word)\n\nfor chunk_start in range(0, n_secs+1, seconds_per_chunk):\n    chunk_end = min(chunk_start + seconds_per_chunk, n_secs+1)\n    chunk_words = []\n    for s in range(chunk_start, chunk_end):\n        chunk_words.extend(words_by_sec[s])\n    start_time = f\"{chunk_start//60:02d}:{chunk_start%60:02d}\"\n    end_time = f\"{(chunk_end-1)//60:02d}:{(chunk_end-1)%60:02d}\"\n    transcript = ' '.join(chunk_words).strip()\n    chunks.append({\n        'start_time': start_time,\n        'end_time': end_time,\n        'transcript': transcript\n    })\n\ndf_chunks = pd.DataFrame(chunks)\ncsv_filename = f\"transcript_{video_filename.split('.')[0]}_10sec_chunks.csv\"\ndf_chunks.to_csv(csv_filename, index=False)\nprint(f\"\\nTranscript chunks saved to CSV as: {csv_filename}\")\n\n# 6. Chunk on 10-minute basis and summarize\nfrom transformers import pipeline, AutoTokenizer\n\nprint(\"Loading summarization model...\")\nmodel_name = \"facebook/bart-large-cnn\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsummarizer = pipeline(\"summarization\", model=model_name, device=0)\n\nten_min_chunks = []\ntext_chunks = df_chunks['transcript'].tolist()\nchunk_size = 10 * 60 // seconds_per_chunk  # number of 10sec chunks per 10 min = 60\n\nfor i in range(0, len(text_chunks), chunk_size):\n    chunk_start_sec = i * seconds_per_chunk\n    chunk_end_sec = min((i + chunk_size) * seconds_per_chunk, n_secs+1)\n    chunk_text = ' '.join(text_chunks[i:i+chunk_size])\n    ts_label = f\"{chunk_start_sec//60:02d}:{chunk_start_sec%60:02d} - {chunk_end_sec//60:02d}:{chunk_end_sec%60:02d}\"\n    tokens = tokenizer.encode(chunk_text)\n    max_tokens = 900\n    if len(tokens) > max_tokens:\n        subnotes = []\n        for j in range(0, len(tokens), max_tokens):\n            sub_chunk = tokenizer.decode(tokens[j:j+max_tokens], skip_special_tokens=True)\n            summary = summarizer(sub_chunk, max_length=200, min_length=60, do_sample=False)[0]['summary_text']\n            subnotes.append(summary)\n        summary_full = \" \".join(subnotes)\n    else:\n        summary_full = summarizer(chunk_text, max_length=200, min_length=60, do_sample=False)[0]['summary_text']\n    ten_min_chunks.append(f\"=== üìÖ Time {ts_label} ===\\nüìù {summary_full}\\n\")\n    print(f\"Summarized chunk: {ts_label}\")\n\nfinal_notes = \"\\n\".join(ten_min_chunks)\nchunked_notes_filename = f\"notes_{video_filename.split('.')[0]}_10minchunks.txt\"\nwith open(chunked_notes_filename, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"10-MINUTE CHUNKED VIDEO NOTES\\n\")\n    f.write(\"=\"*60 + \"\\n\\n\")\n    f.write(final_notes)\nprint(f\"\\nChunked notes saved as: {chunked_notes_filename}\")\n\nprint(\"\\n‚úÖ All steps complete. CSV and summaries are ready!\")\n\n# (Optional) Preview CSV file\nprint(df_chunks.head)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:27:23.688928Z","iopub.execute_input":"2025-10-05T09:27:23.689788Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nFound video: CSE-Math4641-Lecture18(Recording)1.mkv at /kaggle/input/rifatt/CSE-Math4641-Lecture18(Recording)1.mkv\nExtracting audio from video...\nAudio extraction completed! Video duration: 4073.07 seconds\nLoading Whisper model...\nTranscribing audio with word-level timestamps...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}