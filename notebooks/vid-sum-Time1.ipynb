{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13266612,"sourceType":"datasetVersion","datasetId":8407008}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==== Kaggle Video-to-Notes Pipeline with Per-Second Timestamp & 10-Minute Summaries ====\n\n# 1. Install libraries (run once)\n!pip install moviepy git+https://github.com/openai/whisper.git transformers torch --quiet\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 2. Upload your video using Kaggle's sidebar \"Add data\" (Settings > Data > +Upload)\n# The file will appear at /kaggle/input/{dataset-name}/{video-filename}\n\nimport os\ninput_dir = '/kaggle/input/rifatt'  # Change if your file is elsewhere\nvideo_filename = 'CSE-Math4641-Lecture18(Recording)1.mkv'          # <- Set to your filename, e.g. 'lecture.mp4'\nfor root, dirs, files in os.walk(input_dir):\n    for file in files:\n        if file.lower().endswith(('.mp4', '.mkv', '.mov', '.avi')):\n            video_filename = file\n            video_path = os.path.join(root, file)\nprint(f\"Found video: {video_filename} at {video_path}\")\n\n# 3. Extract audio from video\nfrom moviepy.editor import VideoFileClip\n\nprint(\"Extracting audio from video...\")\nvideo = VideoFileClip(video_path)\naudio_path = './extracted_audio.wav'\nvideo.audio.write_audiofile(audio_path, verbose=False, logger=None)\nprint(\"Audio extraction completed!\")\n\n# 4. Transcribe audio with Whisper (word timestamps for per-second mapping)\nimport whisper\n\nprint(\"Loading Whisper model...\")\nmodel = whisper.load_model('base')  # Use 'small' or 'medium' for better accuracy if RAM permits\nprint(\"Transcribing audio with word-level timestamps...\")\nresult = model.transcribe(audio_path, word_timestamps=True)\nprint(\"Transcription completed!\")\n\n# 5. Build per-second transcript\nfrom collections import defaultdict\nimport math\n\nvideo_length = math.ceil(result['segments'][-1]['end'])\nsecond_to_words = defaultdict(list)\n\nfor segment in result['segments']:\n    for word_info in segment.get('words', []):\n        word = word_info['word']\n        start_sec = int(word_info['start'])\n        end_sec = int(word_info['end'])\n        for s in range(start_sec, end_sec+1):\n            second_to_words[s].append(word)\n\nper_second_text = []\nfor sec in range(video_length+1):\n    text = ' '.join(second_to_words[sec])\n    per_second_text.append(f\"Second {sec:04d}: {text}\")\n\n# Save full per-second transcript\nper_second_filename = \"per_second_transcript.txt\"\nwith open(per_second_filename, \"w\", encoding=\"utf-8\") as f:\n    f.write('\\n'.join(per_second_text))\nprint(f\"Per-second transcript saved as: {per_second_filename}\")\n\n# 6. Chunk transcript into 10-minute blocks and summarize\nfrom transformers import pipeline, AutoTokenizer\n\nprint(\"Loading summarization model...\")\nmodel_name = \"facebook/bart-large-cnn\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsummarizer = pipeline(\"summarization\", model=model_name, device=0)\n\ndef concat_text_for_interval(second_to_words, start_sec, end_sec):\n    chunk_words = []\n    for sec in range(start_sec, end_sec):\n        chunk_words.extend(second_to_words[sec])\n    return ' '.join(chunk_words)\n\nchunk_duration = 10 * 60  # 10 mins in seconds\n\nchunked_notes = []\nfor chunk_start in range(0, video_length+1, chunk_duration):\n    chunk_end = min(chunk_start + chunk_duration, video_length+1)\n    chunk_text = concat_text_for_interval(second_to_words, chunk_start, chunk_end)\n    timestamp_label = f\"{chunk_start//60:02d}:{chunk_start%60:02d} - {chunk_end//60:02d}:{chunk_end%60:02d}\"\n    tokens = tokenizer.encode(chunk_text)\n    max_tokens = 900\n    if len(tokens) > max_tokens:\n        subnotes = []\n        for i in range(0, len(tokens), max_tokens):\n            sub_chunk = tokenizer.decode(tokens[i:i+max_tokens], skip_special_tokens=True)\n            summary = summarizer(sub_chunk, max_length=200, min_length=60, do_sample=False)[0]['summary_text']\n            subnotes.append(summary)\n        summary_full = \" \".join(subnotes)\n    else:\n        summary_full = summarizer(chunk_text, max_length=200, min_length=60, do_sample=False)[0]['summary_text']\n\n    chunked_notes.append(\n        f\"=== üìÖ Time {timestamp_label} ===\\nüìù {summary_full}\\n\"\n    )\n    print(f\"Summarized chunk: {timestamp_label}\")\n\nfinal_chunked_notes = \"\\n\".join(chunked_notes)\n\nchunked_notes_filename = f\"notes_{video_filename.split('.')[0]}_10minchunks.txt\"\nwith open(chunked_notes_filename, 'w', encoding='utf-8') as f:\n    f.write(\"10-MINUTE CHUNKED VIDEO NOTES\\n\")\n    f.write(\"=\"*60 + \"\\n\\n\")\n    f.write(final_chunked_notes)\nprint(f\"Chunked notes saved as: {chunked_notes_filename}\")\n\n# 7. Print summary preview\nprint(\"\\n\" + \"=\"*60)\nprint(\"           üìö 10-MINUTE CHUNKED VIDEO NOTES\")\nprint(\"=\"*60)\nprint(final_chunked_notes)  # Print only a preview\n\nprint(\"\\n‚úÖ All steps complete. Files are ready for LMS integration.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T08:20:31.615946Z","iopub.execute_input":"2025-10-05T08:20:31.616660Z","iopub.status.idle":"2025-10-05T08:24:16.898934Z","shell.execute_reply.started":"2025-10-05T08:20:31.616629Z","shell.execute_reply":"2025-10-05T08:24:16.898046Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nFound video: CSE-Math4641-Lecture18(Recording)1.mkv at /kaggle/input/rifatt/CSE-Math4641-Lecture18(Recording)1.mkv\nExtracting audio from video...\nAudio extraction completed!\nLoading Whisper model...\nTranscribing audio with word-level timestamps...\nTranscription completed!\nPer-second transcript saved as: per_second_transcript.txt\nLoading summarization model...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Summarized chunk: 00:00 - 10:00\n","output_type":"stream"},{"name":"stderr","text":"Your max_length is set to 200, but your input_length is only 111. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n","output_type":"stream"},{"name":"stdout","text":"Summarized chunk: 10:00 - 20:00\nSummarized chunk: 20:00 - 30:00\nSummarized chunk: 30:00 - 40:00\nSummarized chunk: 40:00 - 50:00\nSummarized chunk: 50:00 - 60:00\nSummarized chunk: 60:00 - 67:52\nChunked notes saved as: notes_CSE-Math4641-Lecture18(Recording)1_10minchunks.txt\n\n============================================================\n           üìö 10-MINUTE CHUNKED VIDEO NOTES\n============================================================\n=== üìÖ Time 00:00 - 10:00 ===\nüìù Assalamualaikum: The plan  for  for today  is  to  revisit  to the core  core concepts  of  of linear algebra. The  first  of the  the  concepts  is the  a new  new decomposition. We'll  try  to solve  solve  an  example. So,  that's  the essence  essence  of  iluting  Iluting  composition is  we  we are going to  to take  the  matrix  matrix. matrix  and  we're going  to decompose  it  into  into its inherent  lower  lower and lower triangular  triangular  matrices. And  what's the advantage  advantage  of doing  this?  This? So,  you  you guys have already  already learned  learned  about Gaussian  Gaussian elimination  that  in  your linear  algebra  algebra groups. The ALU  ALU decomposition helps us to record the steps  steps  of the  Gaussian  elimination  elimination again. So,  what  what does  these  terms mean?  We  will  come  come to  we  will discuss  the  the mean of  matrix  A  A, A A,  A A   A - A. So,  this  thing  thing is  our  original  original matrix  matrix  A. So,  our goal  is  to decompose  decompose this  Matrix  matrix into  its  inherent lower  lower  triangular  triangular and  upper  triangular¬† triangular  matrices. We need  to get  zeros  at  at the  lower¬† lower  triangle  triangle right. All  entries  below  below the  diagonal  has  to  has to  come to  become  0. First  first  row  row will remain  as  it  is. We are not making  making  any changes  to  our first  first row  because  because it  does  not concern  concern  our upper  upper  triangular  triangular matrix. So,  this  thing  is  is the  matrix that  we  we get  after  eliminating  eliminating our L3  L3.\n\n=== üìÖ Time 10:00 - 20:00 ===\nüìù The creation  creation  of these elimination  elimination  matrices are pretty  pretty  simple  very intuitive. So,  our elimination  matrix  matrix had to  to be multiplied at  at  the  left  left left  side of the  original  side  if  you recall. We  will  keep  this  in our  our memory which  which we keep  in  our record so  so that  we do  not  need  to perform  perform  portion  portion elimination every  every single time. So,  what  did  we  do?  We  took  minus  2  2 of  our  our first  row. We  added  it  with  1  times  times r  r  2 right. So,  the  in  this  this entry  over  here  here we  put  0  ok. This  is row  3  3 does  not  concern  itself with  with the  final  final value of  row  row 2. We need to convert  this  2  to  0. So,  how  can  we do that? That is, minus  2 plus  plus  2 would become  become 0. If  I  take  1  instance of  of  rho  1 and  add  it  it with  Rho  3  I would end  up  up with  1 over  here  here. Say  Say  2  times  rho  2. So,  the  value  value at  this  entry  entry is  0  right. And  12 plus  plus  minus  10  10 is  12  12 minus 10  which  which is  2 right. We  end  up with 2  2 over  over  here  ok. Now  the next target is to come  come up with the  elimination  elimination elimination matrix. So,  in our original  original  matrix  we had  1  1 minus  2  and  3  3 in our resultant  resultant  matrix. Now,  we will  will  also have  1, 1 minus 2 and 3  in  the first  first  row. So,  how do  do  we store  that  that in our elimination matrix?  matrix? We  We can  say  ok  we  took  0  times  times. our  first, first, row, and  2, 2, times  our  second, second, and third, third. matrix. We  take  1 instance  instance of  the  first row  row and 0  instance  of the  second and third. We have  come  up with  with our  upper  upper triangular  matrix  matrix. Now  is  we  want  to find  out  our  our value  of the elimination matrices  matrices at  our disposal  disposal.   All   all    the  elimination  mat matrix matrices are at our disposal and   and  we have  also seen  seen  that  we are  better than  other  people.\n\n=== üìÖ Time 20:00 - 30:00 ===\nüìù So,  how  can we find  out  out the value  of  L? So,  why not  not  take  all  all of  these  elimination  matrices  and  take them  to the  the  left  hand  side of  the big relation  relation  here  here and  and we take  it to the left  ok. So, ultimately  if  if I  I  multiply  all the all  these matrices together  gives  gives me  A  we can say  say that  ok  L  L is  the lower  triangular  triangular matrix  right.   right  of L. The lower  lower  triangular  matrix  matrix is  basically  the  the multiplication  multiplication of  of the  elimination  elimination elimination  matrices. So,  for  let  us say  say  E  2  1  1 or   or  or a target  target  so,  find  the inverse  of   of E 2  1 - E  E sub  3  3 - 2. So,  simply  I  I can  just  it  it may be  use  a  different  color.  Now,  if  I multiply  multiply  them  together I  should  get  something  like  1. So,  since  since I  have  minus  2  over  here  here I  am  going  to  put  minus¬† 2  2 over here. And  and  what  this  is giving  me  me is  the  L  L matrix  matrix  right. Now  let  us head  back  back and match  our answers  ok. Eigenvalues  eigenvalues and eigenvectors are basically  what  all  all the vectors  vectors  in  the  vectors. Eigenvector are what  are  they. So,  these are very  abstract  abstract concepts  concepts  very  hard  hard to impart  impart  in pen  pen  and paper right. Now that we know  how  to  perform LUD  LUD composition, let  let us talk  about the  the last topic of this  this  course. There are infinite  infinite  number  of  such  vectors  vectors right. So,  if  you  have  a  particular  particular matrix  matrix  A  let  us try  to  understand  how  we can  determine  determine the  the eigenvector  eigen vector  and  eigenevalues  of our particular transformation  transformation  matrix A  ok. This  pink  pink or  this  purple  colored  vector  is  not  an  eiken vector  because  it  is changing  changing  its  direction  direction.\n\n=== üìÖ Time 30:00 - 40:00 ===\nüìù Our vector  V is  is  an eigen vector and  our scalar  scalar is the  eigenvalue  of  that  corresponding  corresponding eigenvector. So,  what  does this  this thing telling  you? So, let  us head  back  back to  our bamboo  paper  paper and  try  to understand  it. Eigenvalue  basically dictates  dictates  the degree  degree  to which  our  our transformation  transformation  matrix  matrix stretches or squishes. So,  how  are  are we going to convert  a  particular  particular scalar  value  value to its  corresponding  corresponding matrix  Matrix  matrix. We end  up with something  something like this. So,  what  did  we do  we saw  saw  that  if  we  had  an  equation  that would  would  something  like  this. So,  of  course,  a  vector  vector equals  to  0  0. Now,  our  our transformation  matrix  matrix a  a has to  to be  sorry  not  not the  transformation  it  is  the  the matrix that  I  I am multiplying  with. So, if  if  I  have  a  singular  singular matrix  there  there will  be a  theorem  that says  that  the  determinant  of  that matrix  matrix  is  0  0 again. So,  instead of  of a ¬†determinant    like  this  we can simply  say  say that ¬†the¬†determine¬†of  a¬† a minus  minus  ¬†a   i  i is  equal  to  to 0. And  again  let  me  me expand  it  a bit  bit  further  let‚Äôs say   our  our a  a would look  look  something  like this  right  right. So,  in  in our linear  algebra  algebra course  we went  went  a bit  more  in depth  right. But  for  this  this course  this amount  of  of superficial  knowledge  knowledge is  enough  ok  ok. And  another  thing  another another  terminology  that  that I  have  let  to  point  out  out. So,  this is  called  called the  characteristic  characteristic equation  equation.\n\n=== üìÖ Time 40:00 - 50:00 ===\nüìù The word  Eigen is a German word that means proper  proper  to  get  a characteristic,  or  a property of  the original matrix. The solution  solution to the equation  equation is called the characteristic  characteristic equation. The Eigen vectors are the vectors that do  not  change  their  span or their direction. So,  instead of  going  through  all  these  mathematical  mathematical challenge  let  us try  to solve  solve  it in our bamboo  paper. So,  our  our target is to find  out  out the  Eigen  values  values and Eigen vectors. We  need  to come  up with solutions  solutions for  for this equation  equation. So,  in  the  diagonal  diagonal the  the diagonal  entries  entries of  our  identity  matrix  matrix is  is  all all  1  right  right. And  if  we multiply  multiply  Lambda  with  them  we get  just  just Lambda itself  itself right. So,  this  is our  determinant  determinants  over  here  here. So  we are going  to get  two  different  sets  of  eigenvectors  for each of  these  these two  two eigenvalues. So  let's  start  out  out the  set of eigenevectors for  for   for the first time. Okay. We  have  minus  minus. 3 times,  times, 4 times, and  4  4 remaining  4.   This  whole  thing  is  equal equal  to  0. So,  this  thing is  our characteristic  characteristic  equation. Now  let  let me use  my  calculator  and  find  find out  the roots  roots of  this quadratic  quadratics equation. In our original  original equation,  equation, we  have  the  the matrix  matrix  1  1 minus  minus  7,  7. So  let's  write  write for  lambda  Lambda  1 is  is  equal  to 7. But  we have,  have, and  we  need  to  subtract    I,  okay, Okay,  which  means  7 times  times  1, which  is is 7. 4  4 will remain  as  as it  is.\n\n=== üìÖ Time 50:00 - 60:00 ===\nüìù In order to perform Gaussian elimination,  first  order  of  things  things is  2,  a form and  and augmented  matrix,  matrix. So  this  is  is my augmented  augmented matrix. Let  us  us perform  the  the Gaussian  Gaussian¬† elimination  on this  this matrix.  matrix?  It  says  says that  whatever equation  equation we form using  using  this  first  first row over  here,  and  so  we are going to  to  here for  for more space. So  whatever  whatever we have  over  over. here  is  minus  minus 6x  6x plus  plus  4x,  4X,  it  it gives  me  0,  right.  So  there  there are  an  infinite  infinite number of  solutions  solutions to  this equation,  equation, right. So,  we  can  say that  for  our eigenvalue  eigen value  number  1,  1. We  get eigen vector   that  that looks  something  like  this. So,  is  is the  the set  of Eigen  Eigen vectors  finite  finite of  of  course,  not  if  if I use  use  any  other  scalar  multiple  of this  this  vector. So,  this  is  my  my matrix  matrix which  which  when  multiplied  multiplied with  my my  Eigen vector  vector  V  would  give  give me  a  value  value of  0  right. So,  let  us go  to  to the  next page. We have an  infinite  number  number of  solutions  solutions for  this  this equation  right. So,  what  we  can  do  do is  we can  come  up with  with  a  particular  particular valid  pair  of  xy  Xy  value  values. That  basically  basically suffice  suffice  or  satisfies  satisfies this  equation right  here.\n\n=== üìÖ Time 60:00 - 67:52 ===\nüìù So,  what  can  we do  do  over  here?  Can  say  say that  if  I  take  the  the value  of  x  x and   y  to be  be  1  I should  get   minus  2  2 times  2 - 4. So, minus  4 plus  plus  4 basically  we nullify  nullify each  other. And  we  end  up with  0. So,  if  we had a  particular  particular vector  that  that was  something  like  this  it  would  would be  one  upon  being  transformed  transformed. So,  this is  my  my v2  v2 prime. The length  of  the  the vector  will  remain  as  as it  is okay. So,  for example,  they got  14  14 and  21  as  as the result in vector  vector  right. So,  if  we multiply  2  2 by  7 and  and  3  by 7  we  we end  with  14, 14 and 21 right. Now,  we can  of  course,  verify  and check  check  whether  the answers  answers  that  we have got  are correct  or not. So,  we can  say  this  minus  minus 1  that  we  we got  was  the  second  second eigenvalue  eigen value  right  and  and we  are  2. Okay,  I  hope  I hope  you  guys understood  understood  this. And  that's the  end  end of  this lecture  lecture. If  if you  have  any question  question  about  about this  lecture  or  any  other lectures  lectures  preceding  this¬† lecture  you can  of  course compact  compact  me  me through  email,  messenger, or  or WhatsApp.\n\n\n‚úÖ All steps complete. Files are ready for LMS integration.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}